{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.densenet169(pretrained=True)\n",
    "model.classifier = nn.Sequential(nn.Linear(1664, 121))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: initialize the hyperparameters/variables\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 10           # Number of full passes through the dataset\n",
    "batch_size = 128         # Number of samples in each minibatch\n",
    "learning_rate = 0.01  \n",
    "seed = np.random.seed(0) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "\n",
    "#TODO: Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(224),\n",
    "        transforms.ToPILImage('L'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize([224,224],interpolation=2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n",
      "Model on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "model = model\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### need to modify\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "from plankton_dataloader import create_split_loaders\n",
    "root_dir = \"../dataset/data_subset/\"\n",
    "train_loader, val_loader, test_loader = create_split_loaders(root_dir,batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(computing_device)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader,model,optimizer):\n",
    "    start = time.time()\n",
    "    sum_loss = 0.0\n",
    "    sum_accuracy = 0.0\n",
    "    list_sum_loss = []\n",
    "    num = 0\n",
    "    for mb_count, (val_images, val_labels) in enumerate(val_loader, 0):\n",
    "        model.eval()\n",
    "        with torch.no_grad():  \n",
    "            optimizer.zero_grad()      \n",
    "            val_images = torch.squeeze(torch.stack([val_images,val_images,val_images], dim=1, out=None))\n",
    "            val_images, val_labels = val_images.to(computing_device), val_labels.to(computing_device)\n",
    "            val_labels = val_labels.type(torch.cuda.FloatTensor)\n",
    "            outputs = model(val_images)\n",
    "            loss = criterion(outputs,torch.max(val_labels, 1)[1])\n",
    "            sum_loss += loss\n",
    "            output_np = outputs.cpu().detach().numpy()\n",
    "            label_np = val_labels.cpu().detach().numpy()\n",
    "\n",
    "            accuracy_train = accuracy(label_np, output_np)\n",
    "            sum_accuracy += accuracy_train\n",
    "            #print('validation accuracy',accuracy_train)\n",
    "    print(\"Vali Loss: {}, Vali Accuracy: {}\".format(1.0 * sum_loss / mb_count, sum_accuracy / mb_count))\n",
    "    print(\"validation time = \", time.time()-start)    \n",
    "    return (1.0*sum_loss / mb_count).item(), sum_accuracy / mb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    lab = np.argmax(labels, axis=1)\n",
    "    return np.sum(outputs==lab)/float(lab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10,resume=False, direct=\"\"):\n",
    "    since = time.time()\n",
    "    total_loss = []\n",
    "    if resume == False:\n",
    "        avg_minibatch_loss = []\n",
    "        avg_minibatch_accuracy = []\n",
    "        total_vali_loss = []\n",
    "        total_vali_accuracy = []\n",
    "        epc_save = 0 # MAYBE\n",
    "    else: \n",
    "        print('Resume model: %s' % direct)\n",
    "        check_point = torch.load(direct)\n",
    "        model.load_state_dict(check_point['state_dict'])\n",
    "        optimizer.load_state_dict(check_point['optimizer'])\n",
    "        avg_minibatch_loss = list(np.load('avg_train_loss.npy'))\n",
    "        avg_minibatch_accuracy = list(np.load('avg_train_accuracy.npy'))\n",
    "        total_vali_loss = list(np.load('total_vali_loss.npy'))\n",
    "        total_vali_accuracy = list(np.load('total_vali_accuracy.npy'))\n",
    "        epc_save = check_point['epoch'] - 1 \n",
    "    \n",
    "    tolerence = 3\n",
    "    i = 0 \n",
    "    for epoch in range(epc_save, num_epochs):\n",
    "        N = 20\n",
    "        M = 20\n",
    "        N_minibatch_loss = 0.0    \n",
    "        best_loss = 100\n",
    "        early_stop = 0\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            scheduler.step()\n",
    "            # Iterate over data.\n",
    "            for minibatch_count, (inputs, labels) in enumerate(train_loader, 0):\n",
    "                inputs = torch.squeeze(torch.stack([inputs,inputs,inputs], dim=1, out=None))\n",
    "                inputs = inputs.to(computing_device)\n",
    "                labels = labels.to(computing_device)\n",
    "                labels = labels.long()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    labels = labels.long()\n",
    "                    loss = criterion(outputs,torch.max(labels, 1)[1])\n",
    "                    #loss = criterion(torch.max(outputs,1)[1],torch.max(labels, 1)[1])\n",
    "                    N_minibatch_loss += loss\n",
    "                    # backward + optimize only if in training phase\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                # training stats\n",
    "                if minibatch_count % N == 0 and minibatch_count!=0:    \n",
    "\n",
    "                    # Print the loss averaged over the last N mini-batches    \n",
    "                    N_minibatch_loss /= N\n",
    "                    print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                        (epoch + 1, minibatch_count, N_minibatch_loss))\n",
    "\n",
    "                    # Add the averaged loss over N minibatches and reset the counter\n",
    "                    avg_minibatch_loss.append(N_minibatch_loss)\n",
    "                    \n",
    "                    avg_minibatch_loss_1 = np.array(avg_minibatch_loss)\n",
    "                    np.save('avg_train_loss', avg_minibatch_loss_1)\n",
    "                    \n",
    "                    N_minibatch_loss = 0.0\n",
    "\n",
    "                    output_np = outputs.cpu().detach().numpy()\n",
    "                    label_np = labels.cpu().detach().numpy()\n",
    "\n",
    "                    accuracy_train = accuracy(label_np, output_np)\n",
    "                    avg_minibatch_accuracy.append(accuracy_train)\n",
    "                    np.save('avg_train_accuracy', np.array(avg_minibatch_accuracy))\n",
    "                    \n",
    "                    print('accuracy',accuracy_train)\n",
    "                    #print('accuracy, precision, recall', accuracy, precision, recall)\n",
    "                \n",
    "                #Validation\n",
    "                if minibatch_count % M == 0 and minibatch_count!=0: \n",
    "                    #model = torch.load('./checkpoint')\n",
    "                    save_checkpoint({'epoch': epoch + 1,\n",
    "                                'state_dict': model.state_dict(),\n",
    "                                'optimizer': optimizer.state_dict(),\n",
    "                                },\n",
    "                                filename='./checkpoint/'+'%d_model_epoch%d.pth' % (epoch , minibatch_count))\n",
    "                    \n",
    "                    v_loss, v_accuracy = validate(val_loader,model,optimizer)\n",
    "#                     print(v_loss)\n",
    "                    \n",
    "                    # Save validation loss and accuracy\n",
    "                    \n",
    "                    total_vali_loss.append(v_loss)\n",
    "                    total_vali_loss_1 = np.array(total_vali_loss)\n",
    "                    np.save('total_vali_loss', total_vali_loss_1)                    \n",
    "                    total_vali_accuracy.append(v_accuracy)\n",
    "                    np.save('total_vali_accuracy', np.array(total_vali_accuracy))\n",
    "                    \n",
    "                    if total_vali_loss[i] > best_loss and i != 0:\n",
    "                        early_stop += 1\n",
    "                        if early_stop == tolerence:\n",
    "                            print('early stop here')\n",
    "                            break\n",
    "                    else:\n",
    "                        best_loss = total_vali_loss[i] \n",
    "                        early_stop = 0\n",
    "                    i = i + 1\n",
    "            print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "    print(\"Training complete after\", epoch, \"epochs\")\n",
    "    \n",
    "    avg_minibatch_loss = np.array(avg_minibatch_loss)\n",
    "    np.save('avg_minibatch_loss_new', avg_minibatch_loss)\n",
    "\n",
    "    total_vali_loss = np.array(total_vali_loss)\n",
    "    np.save('total_vali_loss_new', total_vali_loss)  \n",
    "    print(\"total_vali_loss\")\n",
    "    print(total_vali_loss)\n",
    "    print(\"avg_minibatch_loss\")\n",
    "    print(avg_minibatch_loss)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s '.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best=0, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume model: ./checkpoint/7_model_epoch40.pth\n",
      "Epoch 7/9\n",
      "----------\n",
      "Epoch 8, average minibatch 20 loss: 1.137\n",
      "accuracy 0.8046875\n",
      "Vali Loss: 0.8354580402374268, Vali Accuracy: 0.8286097032242306\n",
      "validation time =  22.61225914955139\n",
      "Epoch 8, average minibatch 40 loss: 0.543\n",
      "accuracy 0.78125\n",
      "Vali Loss: 0.5235973596572876, Vali Accuracy: 0.8740191438690768\n",
      "validation time =  21.87377667427063\n",
      "Epoch 8, average minibatch 60 loss: 0.507\n",
      "accuracy 0.828125\n",
      "Vali Loss: 0.5340747237205505, Vali Accuracy: 0.8669508732291158\n",
      "validation time =  27.340603351593018\n",
      "Epoch 8, average minibatch 80 loss: 0.534\n",
      "accuracy 0.8203125\n",
      "Vali Loss: 0.5185205340385437, Vali Accuracy: 0.869370572789448\n",
      "validation time =  25.759816646575928\n",
      "Epoch 8, average minibatch 100 loss: 0.485\n",
      "accuracy 0.8671875\n",
      "Vali Loss: 0.5200827121734619, Vali Accuracy: 0.8676722032242306\n",
      "validation time =  25.804400205612183\n",
      "Epoch 8, average minibatch 120 loss: 0.521\n",
      "accuracy 0.8359375\n",
      "Vali Loss: 0.5420551300048828, Vali Accuracy: 0.8561652723497801\n",
      "validation time =  25.662578582763672\n",
      "Epoch 8, average minibatch 140 loss: 0.516\n",
      "accuracy 0.8515625\n",
      "Vali Loss: 0.5619303584098816, Vali Accuracy: 0.8556767525647289\n",
      "validation time =  27.56363081932068\n",
      "Epoch 8, average minibatch 160 loss: 0.493\n",
      "accuracy 0.828125\n",
      "Vali Loss: 0.5215262770652771, Vali Accuracy: 0.8709200964826576\n",
      "validation time =  23.24952459335327\n",
      "Finished 8 epochs of training\n",
      "Epoch 8/9\n",
      "----------\n",
      "Epoch 9, average minibatch 20 loss: 0.386\n",
      "accuracy 0.9296875\n",
      "Vali Loss: 0.5362808704376221, Vali Accuracy: 0.873362695407914\n",
      "validation time =  22.3608078956604\n",
      "Epoch 9, average minibatch 40 loss: 0.409\n",
      "accuracy 0.8515625\n",
      "Vali Loss: 0.5415525436401367, Vali Accuracy: 0.8638938080117246\n",
      "validation time =  21.979311227798462\n",
      "Epoch 9, average minibatch 60 loss: 0.390\n",
      "accuracy 0.8515625\n",
      "Vali Loss: 0.5782573223114014, Vali Accuracy: 0.8605810332193454\n",
      "validation time =  25.06892442703247\n",
      "Epoch 9, average minibatch 80 loss: 0.387\n",
      "accuracy 0.8671875\n",
      "Vali Loss: 0.4993654489517212, Vali Accuracy: 0.8788394601856375\n",
      "validation time =  24.15717124938965\n",
      "Epoch 9, average minibatch 100 loss: 0.416\n",
      "accuracy 0.9375\n",
      "Vali Loss: 0.5230196118354797, Vali Accuracy: 0.864233481924768\n",
      "validation time =  26.40620231628418\n",
      "Epoch 9, average minibatch 120 loss: 0.409\n",
      "accuracy 0.8828125\n",
      "Vali Loss: 0.5117322206497192, Vali Accuracy: 0.8771410906204202\n",
      "validation time =  25.68768620491028\n",
      "Epoch 9, average minibatch 140 loss: 0.381\n",
      "accuracy 0.84375\n",
      "Vali Loss: 0.5393181443214417, Vali Accuracy: 0.8659318514899854\n",
      "validation time =  24.626447677612305\n",
      "Epoch 9, average minibatch 160 loss: 0.424\n",
      "accuracy 0.8359375\n",
      "Vali Loss: 0.5552604794502258, Vali Accuracy: 0.8633213238886175\n",
      "validation time =  24.924593448638916\n",
      "Finished 9 epochs of training\n",
      "Epoch 9/9\n",
      "----------\n",
      "Epoch 10, average minibatch 20 loss: 0.320\n",
      "accuracy 0.9296875\n",
      "Vali Loss: 0.5528345704078674, Vali Accuracy: 0.8605390510503175\n",
      "validation time =  25.186049938201904\n",
      "Epoch 10, average minibatch 40 loss: 0.311\n",
      "accuracy 0.890625\n",
      "Vali Loss: 0.5525500774383545, Vali Accuracy: 0.8588597642892037\n",
      "validation time =  25.319551706314087\n",
      "Epoch 10, average minibatch 60 loss: 0.261\n",
      "accuracy 0.8828125\n",
      "Vali Loss: 0.5437196493148804, Vali Accuracy: 0.8720879640937957\n",
      "validation time =  26.483269691467285\n",
      "Epoch 10, average minibatch 80 loss: 0.302\n",
      "accuracy 0.921875\n",
      "Vali Loss: 0.5504300594329834, Vali Accuracy: 0.8703895945285783\n",
      "validation time =  26.00926661491394\n",
      "early stop here\n",
      "Finished 10 epochs of training\n",
      "Training complete after 9 epochs\n",
      "total_vali_loss\n",
      "[3.4084692  2.10767579 1.75457621 1.5961982  1.43515897 1.322083\n",
      " 1.29831576 1.1671325  1.13976479 1.12927079 1.10709143 1.08898962\n",
      " 1.10069823 0.98164982 1.07513785 1.00570846 0.9701243  0.97544998\n",
      " 1.01403737 1.02339303 0.98323387 1.01344919 1.01605642 1.01771069\n",
      " 1.38779557 0.78256387 0.80769253 0.73754156 0.74290544 0.80216062\n",
      " 0.77090615 0.73082215 0.68320614 0.76613599 0.72415513 0.70297128\n",
      " 0.71176201 0.70826149 0.76692879 0.71797234 0.76630861 0.71203375\n",
      " 0.74978119 0.75064981 0.76901048 0.75865895 0.76319718 0.75656188\n",
      " 0.78764462 0.78512782 0.83545804 0.52359736 0.53407472 0.51852053\n",
      " 0.52008271 0.54205513 0.56193036 0.52152628 0.53628087 0.54155254\n",
      " 0.57825732 0.49936545 0.52301961 0.51173222 0.53931814 0.55526048\n",
      " 0.55283457 0.55255008 0.54371965 0.55043006]\n",
      "avg_minibatch_loss\n",
      "[tensor(3.9689, device='cuda:0', requires_grad=True)\n",
      " tensor(2.5105, device='cuda:0', requires_grad=True)\n",
      " tensor(1.8398, device='cuda:0', requires_grad=True)\n",
      " tensor(1.5853, device='cuda:0', requires_grad=True)\n",
      " tensor(1.3987, device='cuda:0', requires_grad=True)\n",
      " tensor(1.3410, device='cuda:0', requires_grad=True)\n",
      " tensor(1.2523, device='cuda:0', requires_grad=True)\n",
      " tensor(1.1870, device='cuda:0', requires_grad=True)\n",
      " tensor(1.0181, device='cuda:0', requires_grad=True)\n",
      " tensor(0.9149, device='cuda:0', requires_grad=True)\n",
      " tensor(0.9601, device='cuda:0', requires_grad=True)\n",
      " tensor(0.9151, device='cuda:0', requires_grad=True)\n",
      " tensor(0.8860, device='cuda:0', requires_grad=True)\n",
      " tensor(0.9092, device='cuda:0', requires_grad=True)\n",
      " tensor(0.8374, device='cuda:0', requires_grad=True)\n",
      " tensor(0.8480, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6960, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6568, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6757, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6842, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6219, device='cuda:0', requires_grad=True)\n",
      " tensor(0.5734, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6082, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6504, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6670, device='cuda:0', requires_grad=True)\n",
      " tensor(1.7315, device='cuda:0', requires_grad=True)\n",
      " tensor(0.8619, device='cuda:0', requires_grad=True)\n",
      " tensor(0.7793, device='cuda:0', requires_grad=True)\n",
      " tensor(0.7262, device='cuda:0', requires_grad=True)\n",
      " tensor(0.7457, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6631, device='cuda:0', requires_grad=True)\n",
      " tensor(0.7240, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6971, device='cuda:0', requires_grad=True)\n",
      " tensor(0.5856, device='cuda:0', requires_grad=True)\n",
      " tensor(0.5124, device='cuda:0', requires_grad=True)\n",
      " tensor(0.5628, device='cuda:0', requires_grad=True)\n",
      " tensor(0.5586, device='cuda:0', requires_grad=True)\n",
      " tensor(0.5372, device='cuda:0', requires_grad=True)\n",
      " tensor(0.5420, device='cuda:0', requires_grad=True)\n",
      " tensor(0.6019, device='cuda:0', requires_grad=True)\n",
      " tensor(0.5635, device='cuda:0', requires_grad=True)\n",
      " tensor(0.4318, device='cuda:0', requires_grad=True)\n",
      " tensor(0.4247, device='cuda:0', requires_grad=True)\n",
      " tensor(0.4025, device='cuda:0', requires_grad=True)\n",
      " tensor(0.4778, device='cuda:0', requires_grad=True)\n",
      " tensor(0.3892, device='cuda:0', requires_grad=True)\n",
      " tensor(0.3674, device='cuda:0', requires_grad=True)\n",
      " tensor(0.3900, device='cuda:0', requires_grad=True)\n",
      " tensor(0.3809, device='cuda:0', requires_grad=True)\n",
      " tensor(0.3968, device='cuda:0', requires_grad=True)\n",
      " tensor(0.3968, device='cuda:0', requires_grad=True)\n",
      " tensor(0.3433, device='cuda:0', requires_grad=True)\n",
      " tensor(1.1374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5067, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5336, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4853, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5211, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5165, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4930, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3855, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4092, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3901, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3874, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4155, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4094, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3814, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4236, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3201, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3110, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3022, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "Training complete in 14m 52s \n"
     ]
    }
   ],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10, resume=True, direct = './checkpoint/7_model_epoch40.pth',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3478986 , 0.51854467, 0.57367871, 0.5944179 , 0.64172035,\n",
       "       0.67212308, 0.67490153, 0.69511404, 0.71182294, 0.71154815,\n",
       "       0.71318164, 0.7116741 , 0.70725834, 0.75114115, 0.72534502,\n",
       "       0.74755358, 0.74910311, 0.7542402 , 0.74322179, 0.73680997,\n",
       "       0.75525922, 0.74477131, 0.7297188 , 0.74424081, 0.71216262,\n",
       "       0.80415318, 0.78534135, 0.80238993, 0.8075919 , 0.78880297,\n",
       "       0.79827186, 0.81170997, 0.82134679, 0.79642465, 0.81349231,\n",
       "       0.82499924, 0.81423654, 0.81181684, 0.80372573, 0.81627458,\n",
       "       0.80446996, 0.81468307, 0.8075919 , 0.81393884, 0.81234734,\n",
       "       0.80986276, 0.80313416, 0.80816439, 0.80175256, 0.81355719,\n",
       "       0.8286097 , 0.87401914, 0.86695087, 0.86937057, 0.8676722 ,\n",
       "       0.85616527, 0.85567675, 0.8709201 , 0.8733627 , 0.86389381,\n",
       "       0.86058103, 0.87883946, 0.86423348, 0.87714109, 0.86593185,\n",
       "       0.86332132, 0.86053905, 0.85885976, 0.87208796, 0.87038959])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('./total_vali_accuracy.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
