{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.resnet34(pretrained=True)\n",
    "model.fc = nn.Sequential(nn.Linear(512, 345))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maynot useful \n",
    "class FocalLoss2d(nn.modules.loss._WeightedLoss):\n",
    "\n",
    "    def __init__(self, gamma=2, weight=None, size_average=None, ignore_index=-100,\n",
    "                 reduce=None, reduction='mean', balance_param=0.25):\n",
    "        super(FocalLoss2d, self).__init__(weight, size_average, reduce, reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.size_average = size_average\n",
    "        self.ignore_index = ignore_index\n",
    "        self.balance_param = balance_param\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \n",
    "        # inputs and targets are assumed to be BatchxClasses\n",
    "        assert len(input.shape) == len(target.shape)\n",
    "        assert input.size(0) == target.size(0)\n",
    "        assert input.size(1) == target.size(1)\n",
    "        \n",
    "        weight = Variable(self.weight)\n",
    "           \n",
    "        # compute the negative likelyhood\n",
    "        logpt = - F.binary_cross_entropy_with_logits(input, target, pos_weight=weight, reduction=self.reduction)\n",
    "        pt = torch.exp(logpt)\n",
    "\n",
    "        # compute the loss\n",
    "        focal_loss = -( (1-pt)**self.gamma ) * logpt\n",
    "        balanced_focal_loss = self.balance_param * focal_loss\n",
    "        return balanced_focal_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: initialize the hyperparameters/variables\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 10           # Number of full passes through the dataset\n",
    "batch_size = 128         # Number of samples in each minibatch\n",
    "learning_rate = 0.01  \n",
    "seed = np.random.seed(0) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "\n",
    "#TODO: Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(224),\n",
    "        transforms.ToPILImage('L'),\n",
    "        transforms.Resize([224,224],interpolation=2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "model = model\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### need to modify\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "from plankton_dataloader import create_split_loaders\n",
    "root_dir = \"../dataset/data_subset/\"\n",
    "train_loader, val_loader, test_loader = create_split_loaders(root_dir,batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(computing_device)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader,model,optimizer):\n",
    "    start = time.time()\n",
    "    sum_loss = 0.0\n",
    "    list_sum_loss = []\n",
    "    num = 0\n",
    "    for mb_count, (val_images, val_labels) in enumerate(val_loader, 0):\n",
    "        model.eval()\n",
    "        with torch.no_grad():  \n",
    "            optimizer.zero_grad()      \n",
    "            val_images = torch.squeeze(torch.stack([val_images,val_images,val_images], dim=1, out=None))\n",
    "            val_images, val_labels = val_images.to(computing_device), val_labels.to(computing_device)\n",
    "            val_labels = val_labels.type(torch.cuda.FloatTensor)\n",
    "            outputs = model(val_images)\n",
    "            loss = criterion(outputs,torch.max(val_labels, 1)[1])\n",
    "            sum_loss += loss\n",
    "            output_np = outputs.cpu().detach().numpy()\n",
    "            label_np = val_labels.cpu().detach().numpy()\n",
    "\n",
    "            accuracy_train = accuracy(label_np, output_np)\n",
    "            print('validation accuracy',accuracy_train)\n",
    "    print(\"validation time = \", time.time()-start)    \n",
    "    return 1.0*sum_loss/mb_count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    lab = np.argmax(labels, axis=1)\n",
    "    return np.sum(outputs==lab)/float(lab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "    total_loss = []\n",
    "    avg_minibatch_loss = []\n",
    "    total_vali_loss = []\n",
    "    tolerence = 3\n",
    "    i = 0 \n",
    "    for epoch in range(num_epochs):\n",
    "        N = 100\n",
    "        M = 100\n",
    "        N_minibatch_loss = 0.0    \n",
    "        best_loss = 100\n",
    "        early_stop = 0\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            scheduler.step()\n",
    "            # Iterate over data.\n",
    "            for minibatch_count, (inputs, labels) in enumerate(train_loader, 0):\n",
    "                inputs = torch.squeeze(torch.stack([inputs,inputs,inputs], dim=1, out=None))\n",
    "                inputs = inputs.to(computing_device)\n",
    "                labels = labels.to(computing_device)\n",
    "                labels = labels.long()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    labels = labels.long()\n",
    "                    loss = criterion(outputs,torch.max(labels, 1)[1])\n",
    "                    #loss = criterion(torch.max(outputs,1)[1],torch.max(labels, 1)[1])\n",
    "                    N_minibatch_loss += loss\n",
    "                    # backward + optimize only if in training phase\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                # training stats\n",
    "                if minibatch_count % N == 0 and minibatch_count!=0:    \n",
    "\n",
    "                    # Print the loss averaged over the last N mini-batches    \n",
    "                    N_minibatch_loss /= N\n",
    "                    print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                        (epoch + 1, minibatch_count, N_minibatch_loss))\n",
    "\n",
    "                    # Add the averaged loss over N minibatches and reset the counter\n",
    "                    avg_minibatch_loss.append(N_minibatch_loss)\n",
    "                    \n",
    "                    avg_minibatch_loss_1 = np.array(avg_minibatch_loss)\n",
    "                    np.save('avg_minibatch_loss_new', avg_minibatch_loss_1)\n",
    "                    \n",
    "                    N_minibatch_loss = 0.0\n",
    "\n",
    "                    output_np = outputs.cpu().detach().numpy()\n",
    "                    label_np = labels.cpu().detach().numpy()\n",
    "\n",
    "                    accuracy_train = accuracy(label_np, output_np)\n",
    "                    print('accuracy',accuracy_train)\n",
    "                    #print('accuracy, precision, recall', accuracy, precision, recall)\n",
    "                \n",
    "                #Validation\n",
    "                if minibatch_count % M == 0 and minibatch_count!=0: \n",
    "                    #model = torch.load('./checkpoint')\n",
    "                    save_checkpoint({'epoch': epoch + 1,\n",
    "                                'state_dict': model.state_dict(),\n",
    "                                'optimizer': optimizer.state_dict(),\n",
    "                                },\n",
    "                                filename='./checkpoint/'+'%d_model_epoch%d.pth' % (epoch , minibatch_count))\n",
    "                    v_loss = validate(val_loader,model,optimizer).item()\n",
    "                    print(v_loss)\n",
    "                    total_vali_loss.append(v_loss)\n",
    "\n",
    "                    total_vali_loss_1 = np.array(total_vali_loss)\n",
    "                    np.save('total_vali_loss_new', total_vali_loss_1)                    \n",
    "\n",
    "                    if total_vali_loss[i] > best_loss and i != 0:\n",
    "                        early_stop += 1\n",
    "                        if early_stop == tolerence:\n",
    "                            print('early stop here')\n",
    "                            break\n",
    "                    else:\n",
    "                        best_loss = total_vali_loss[i] \n",
    "                        early_stop = 0\n",
    "                    i = i + 1\n",
    "            print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "    print(\"Training complete after\", epoch, \"epochs\")\n",
    "    \n",
    "    avg_minibatch_loss = np.array(avg_minibatch_loss)\n",
    "    np.save('avg_minibatch_loss_new', avg_minibatch_loss)\n",
    "\n",
    "    total_vali_loss = np.array(total_vali_loss)\n",
    "    np.save('total_vali_loss_new', total_vali_loss)  \n",
    "    print(\"total_vali_loss\")\n",
    "    print(total_vali_loss)\n",
    "    print(\"avg_minibatch_loss\")\n",
    "    print(avg_minibatch_loss)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s '.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best=0, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import display # to display images\n",
    "# for minibatch_count, (inputs, labels) in enumerate(train_loader, 0):\n",
    "#     print(inputs.shape)\n",
    "#     #inputs = inputs.to(computing_device)\n",
    "#     #labels = labels.to(computing_device)\n",
    "#     image = inputs.cpu().detach().numpy()[1].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "#     image = inputs.cpu().detach().numpy()[0].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "#     image = inputs.cpu().detach().numpy()[2].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "    \n",
    "#     image = inputs.cpu().detach().numpy()[3].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "#     #image = Image.fromarray(image.astype(int),\"L\")\n",
    "#     #display(image)\n",
    "#     if minibatch_count >10: \n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> loss = nn.CrossEntropyLoss()\n",
    "# >>> input = torch.randn(3, 5, requires_grad=True)\n",
    "# >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# >>> output = loss(input, target)\n",
    "# >>> output.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
