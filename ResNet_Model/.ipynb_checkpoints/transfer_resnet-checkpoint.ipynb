{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.resnet34(pretrained=True)\n",
    "model.fc = nn.Sequential(nn.Linear(512, 345))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maynot useful \n",
    "class FocalLoss2d(nn.modules.loss._WeightedLoss):\n",
    "\n",
    "    def __init__(self, gamma=2, weight=None, size_average=None, ignore_index=-100,\n",
    "                 reduce=None, reduction='mean', balance_param=0.25):\n",
    "        super(FocalLoss2d, self).__init__(weight, size_average, reduce, reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.size_average = size_average\n",
    "        self.ignore_index = ignore_index\n",
    "        self.balance_param = balance_param\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \n",
    "        # inputs and targets are assumed to be BatchxClasses\n",
    "        assert len(input.shape) == len(target.shape)\n",
    "        assert input.size(0) == target.size(0)\n",
    "        assert input.size(1) == target.size(1)\n",
    "        \n",
    "        weight = Variable(self.weight)\n",
    "           \n",
    "        # compute the negative likelyhood\n",
    "        logpt = - F.binary_cross_entropy_with_logits(input, target, pos_weight=weight, reduction=self.reduction)\n",
    "        pt = torch.exp(logpt)\n",
    "\n",
    "        # compute the loss\n",
    "        focal_loss = -( (1-pt)**self.gamma ) * logpt\n",
    "        balanced_focal_loss = self.balance_param * focal_loss\n",
    "        return balanced_focal_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: initialize the hyperparameters/variables\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 10           # Number of full passes through the dataset\n",
    "batch_size = 128         # Number of samples in each minibatch\n",
    "learning_rate = 0.01  \n",
    "seed = np.random.seed(0) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "\n",
    "#TODO: Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(224),\n",
    "        transforms.ToPILImage('L'),\n",
    "        transforms.Resize([224,224],interpolation=2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n",
      "Model on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "model = model\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### need to modify\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "import quickdraw_dataloader as qd\n",
    "from quickdraw_dataloader import create_split_loaders\n",
    "root_dir = \"../dataset/data_subset/\"\n",
    "train_loader, val_loader, test_loader = create_split_loaders(root_dir,batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(computing_device)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader,model,optimizer):\n",
    "    start = time.time()\n",
    "    sum_loss = 0.0\n",
    "    list_sum_loss = []\n",
    "    num = 0\n",
    "    for mb_count, (val_images, val_labels) in enumerate(val_loader, 0):\n",
    "        model.eval()\n",
    "        with torch.no_grad():  \n",
    "            optimizer.zero_grad()      \n",
    "            val_images = torch.squeeze(torch.stack([val_images,val_images,val_images], dim=1, out=None))\n",
    "            val_images, val_labels = val_images.to(computing_device), val_labels.to(computing_device)\n",
    "            val_labels = val_labels.type(torch.cuda.FloatTensor)\n",
    "            outputs = model(val_images)\n",
    "            loss = criterion(outputs,torch.max(val_labels, 1)[1])\n",
    "            sum_loss += loss\n",
    "            output_np = outputs.cpu().detach().numpy()\n",
    "            label_np = val_labels.cpu().detach().numpy()\n",
    "\n",
    "            accuracy_train = accuracy(label_np, output_np)\n",
    "            print('validation accuracy',accuracy_train)\n",
    "    print(\"validation time = \", time.time()-start)    \n",
    "    return 1.0*sum_loss/mb_count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    lab = np.argmax(labels, axis=1)\n",
    "    return np.sum(outputs==lab)/float(lab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "    total_loss = []\n",
    "    avg_minibatch_loss = []\n",
    "    total_vali_loss = []\n",
    "    tolerence = 3\n",
    "    i = 0 \n",
    "    for epoch in range(num_epochs):\n",
    "        N = 100\n",
    "        M = 100\n",
    "        N_minibatch_loss = 0.0    \n",
    "        best_loss = 100\n",
    "        early_stop = 0\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            scheduler.step()\n",
    "            # Iterate over data.\n",
    "            for minibatch_count, (inputs, labels) in enumerate(train_loader, 0):\n",
    "                inputs = torch.squeeze(torch.stack([inputs,inputs,inputs], dim=1, out=None))\n",
    "                inputs = inputs.to(computing_device)\n",
    "                labels = labels.to(computing_device)\n",
    "                labels = labels.long()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    labels = labels.long()\n",
    "                    loss = criterion(outputs,torch.max(labels, 1)[1])\n",
    "                    #loss = criterion(torch.max(outputs,1)[1],torch.max(labels, 1)[1])\n",
    "                    N_minibatch_loss += loss\n",
    "                    # backward + optimize only if in training phase\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                # training stats\n",
    "                if minibatch_count % N == 0 and minibatch_count!=0:    \n",
    "\n",
    "                    # Print the loss averaged over the last N mini-batches    \n",
    "                    N_minibatch_loss /= N\n",
    "                    print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                        (epoch + 1, minibatch_count, N_minibatch_loss))\n",
    "\n",
    "                    # Add the averaged loss over N minibatches and reset the counter\n",
    "                    avg_minibatch_loss.append(N_minibatch_loss)\n",
    "                    \n",
    "                    avg_minibatch_loss_1 = np.array(avg_minibatch_loss)\n",
    "                    np.save('avg_minibatch_loss_new', avg_minibatch_loss_1)\n",
    "                    \n",
    "                    N_minibatch_loss = 0.0\n",
    "\n",
    "                    output_np = outputs.cpu().detach().numpy()\n",
    "                    label_np = labels.cpu().detach().numpy()\n",
    "\n",
    "                    accuracy_train = accuracy(label_np, output_np)\n",
    "                    print('accuracy',accuracy_train)\n",
    "                    #print('accuracy, precision, recall', accuracy, precision, recall)\n",
    "                \n",
    "                #Validation\n",
    "                if minibatch_count % M == 0 and minibatch_count!=0: \n",
    "                    #model = torch.load('./checkpoint')\n",
    "                    save_checkpoint({'epoch': epoch + 1,\n",
    "                                'state_dict': model.state_dict(),\n",
    "                                'optimizer': optimizer.state_dict(),\n",
    "                                },\n",
    "                                filename='./checkpoint/'+'%d_model_epoch%d.pth' % (epoch , minibatch_count))\n",
    "                    v_loss = validate(val_loader,model,optimizer).item()\n",
    "                    print(v_loss)\n",
    "                    total_vali_loss.append(v_loss)\n",
    "\n",
    "                    total_vali_loss_1 = np.array(total_vali_loss)\n",
    "                    np.save('total_vali_loss_new', total_vali_loss_1)                    \n",
    "\n",
    "                    if total_vali_loss[i] > best_loss and i != 0:\n",
    "                        early_stop += 1\n",
    "                        if early_stop == tolerence:\n",
    "                            print('early stop here')\n",
    "                            break\n",
    "                    else:\n",
    "                        best_loss = total_vali_loss[i] \n",
    "                        early_stop = 0\n",
    "                    i = i + 1\n",
    "            print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "    print(\"Training complete after\", epoch, \"epochs\")\n",
    "    \n",
    "    avg_minibatch_loss = np.array(avg_minibatch_loss)\n",
    "    np.save('avg_minibatch_loss_new', avg_minibatch_loss)\n",
    "\n",
    "    total_vali_loss = np.array(total_vali_loss)\n",
    "    np.save('total_vali_loss_new', total_vali_loss)  \n",
    "    print(\"total_vali_loss\")\n",
    "    print(total_vali_loss)\n",
    "    print(\"avg_minibatch_loss\")\n",
    "    print(avg_minibatch_loss)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s '.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best=0, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "Epoch 1, average minibatch 100 loss: 2.297\n",
      "accuracy 0.625\n",
      "validation accuracy 0.6640625\n",
      "validation accuracy 0.6015625\n",
      "validation accuracy 0.6484375\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.6328125\n",
      "validation accuracy 0.6796875\n",
      "validation accuracy 0.671875\n",
      "validation accuracy 0.640625\n",
      "validation accuracy 0.65625\n",
      "validation accuracy 0.6171875\n",
      "validation accuracy 0.6171875\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.609375\n",
      "validation accuracy 0.609375\n",
      "validation accuracy 0.65625\n",
      "validation accuracy 0.5703125\n",
      "validation accuracy 0.6484375\n",
      "validation accuracy 0.59375\n",
      "validation accuracy 0.59375\n",
      "validation accuracy 0.5703125\n",
      "validation accuracy 0.6015625\n",
      "validation accuracy 0.6796875\n",
      "validation accuracy 0.6875\n",
      "validation accuracy 0.651685393258427\n",
      "validation time =  20.136147022247314\n",
      "1.4079868793487549\n",
      "Finished 1 epochs of training\n",
      "Epoch 1/4\n",
      "----------\n",
      "Epoch 2, average minibatch 100 loss: 1.021\n",
      "accuracy 0.6953125\n",
      "validation accuracy 0.671875\n",
      "validation accuracy 0.703125\n",
      "validation accuracy 0.6484375\n",
      "validation accuracy 0.6796875\n",
      "validation accuracy 0.671875\n",
      "validation accuracy 0.703125\n",
      "validation accuracy 0.6875\n",
      "validation accuracy 0.609375\n",
      "validation accuracy 0.75\n",
      "validation accuracy 0.6484375\n",
      "validation accuracy 0.6796875\n",
      "validation accuracy 0.6796875\n",
      "validation accuracy 0.71875\n",
      "validation accuracy 0.6875\n",
      "validation accuracy 0.6953125\n",
      "validation accuracy 0.71875\n",
      "validation accuracy 0.671875\n",
      "validation accuracy 0.671875\n",
      "validation accuracy 0.6640625\n",
      "validation accuracy 0.6484375\n",
      "validation accuracy 0.6796875\n",
      "validation accuracy 0.625\n",
      "validation accuracy 0.65625\n",
      "validation accuracy 0.6966292134831461\n",
      "validation time =  22.578505277633667\n",
      "1.1435235738754272\n",
      "Finished 2 epochs of training\n",
      "Epoch 2/4\n",
      "----------\n",
      "Epoch 3, average minibatch 100 loss: 0.702\n",
      "accuracy 0.7578125\n",
      "validation accuracy 0.71875\n",
      "validation accuracy 0.71875\n",
      "validation accuracy 0.7265625\n",
      "validation accuracy 0.71875\n",
      "validation accuracy 0.7734375\n",
      "validation accuracy 0.703125\n",
      "validation accuracy 0.6484375\n",
      "validation accuracy 0.6953125\n",
      "validation accuracy 0.703125\n",
      "validation accuracy 0.6875\n",
      "validation accuracy 0.6875\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.65625\n",
      "validation accuracy 0.6796875\n",
      "validation accuracy 0.6484375\n",
      "validation accuracy 0.7421875\n",
      "validation accuracy 0.6328125\n",
      "validation accuracy 0.7109375\n",
      "validation accuracy 0.75\n",
      "validation accuracy 0.7578125\n",
      "validation accuracy 0.671875\n",
      "validation accuracy 0.59375\n",
      "validation accuracy 0.6875\n",
      "validation accuracy 0.7303370786516854\n",
      "validation time =  21.194018840789795\n",
      "1.0700846910476685\n",
      "Finished 3 epochs of training\n",
      "Epoch 3/4\n",
      "----------\n",
      "Epoch 4, average minibatch 100 loss: 0.545\n",
      "accuracy 0.8359375\n",
      "validation accuracy 0.6953125\n",
      "validation accuracy 0.640625\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.6953125\n",
      "validation accuracy 0.71875\n",
      "validation accuracy 0.7265625\n",
      "validation accuracy 0.6484375\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.75\n",
      "validation accuracy 0.6875\n",
      "validation accuracy 0.7421875\n",
      "validation accuracy 0.765625\n",
      "validation accuracy 0.703125\n",
      "validation accuracy 0.7109375\n",
      "validation accuracy 0.6171875\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.6171875\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.7421875\n",
      "validation accuracy 0.703125\n",
      "validation accuracy 0.703125\n",
      "validation accuracy 0.7191011235955056\n",
      "validation time =  22.208549737930298\n",
      "1.0244812965393066\n",
      "Finished 4 epochs of training\n",
      "Epoch 4/4\n",
      "----------\n",
      "Epoch 5, average minibatch 100 loss: 0.427\n",
      "accuracy 0.875\n",
      "validation accuracy 0.71875\n",
      "validation accuracy 0.71875\n",
      "validation accuracy 0.71875\n",
      "validation accuracy 0.7265625\n",
      "validation accuracy 0.796875\n",
      "validation accuracy 0.7421875\n",
      "validation accuracy 0.6953125\n",
      "validation accuracy 0.75\n",
      "validation accuracy 0.7578125\n",
      "validation accuracy 0.7578125\n",
      "validation accuracy 0.65625\n",
      "validation accuracy 0.71875\n",
      "validation accuracy 0.703125\n",
      "validation accuracy 0.6875\n",
      "validation accuracy 0.703125\n",
      "validation accuracy 0.6484375\n",
      "validation accuracy 0.7109375\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.640625\n",
      "validation accuracy 0.7578125\n",
      "validation accuracy 0.75\n",
      "validation accuracy 0.703125\n",
      "validation accuracy 0.734375\n",
      "validation accuracy 0.7191011235955056\n",
      "validation time =  21.698036193847656\n",
      "1.0932117700576782\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 1336) is killed by signal: Killed. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ad6711e93c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-3813e8368b6a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputing_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMP_STATUS_CHECK_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 1336) is killed by signal: Killed. "
     ]
    }
   ],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import display # to display images\n",
    "# for minibatch_count, (inputs, labels) in enumerate(train_loader, 0):\n",
    "#     print(inputs.shape)\n",
    "#     #inputs = inputs.to(computing_device)\n",
    "#     #labels = labels.to(computing_device)\n",
    "#     image = inputs.cpu().detach().numpy()[1].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "#     image = inputs.cpu().detach().numpy()[0].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "#     image = inputs.cpu().detach().numpy()[2].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "    \n",
    "#     image = inputs.cpu().detach().numpy()[3].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "#     #image = Image.fromarray(image.astype(int),\"L\")\n",
    "#     #display(image)\n",
    "#     if minibatch_count >10: \n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> loss = nn.CrossEntropyLoss()\n",
    "# >>> input = torch.randn(3, 5, requires_grad=True)\n",
    "# >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# >>> output = loss(input, target)\n",
    "# >>> output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
